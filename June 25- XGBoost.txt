XG Boost for Regression -eXtreme Gradienet Boost

desinged for large and complicated dataset.

In boosting, the base learners are used in sequentually.

The basic principle behind the working of the boosting algorithm is to generate multiple weak learners and combine their predictions to form one strong rule. 
These weak rules are generated by applying basic Machine Learning algorithms on different distributions of the data set. 
These algorithms generate weak rules for each iteration. 
After multiple iterations, the weak learners are combined to form a strong learner that will predict a more accurate outcome.

Step 1: The base algorithm reads the data and assigns equal weight to each sample observation.

Step 2: False predictions made by the base learner are identified. In the next iteration, these false predictions are assigned to the next base learner with a higher weightage on these incorrect predictions.

Step 3: Repeat step 2 until the algorithm can correctly classify the output.

Therefore, the main aim of Boosting is to focus more on miss-classified predictions.

Types of boosting algorithms
1.Adaptive Boosting or AdaBoost
2.Gradient Boosting
3.XGBoost (Extreme Gradient Boosting)

Gradient Boosting is based on sequential ensemble learning(boosting). 
Here the base learners are generated sequentially in such a way that the present base learner is always more effective than the previous one, 
i.e. the overall model improves sequentially with each iteration.

XGBoost is an advanced version of Gradient boosting method, it literally means eXtreme Gradient Boosting. 
XGBoost developed by Tianqi Chen, falls under the category of Distributed Machine Learning Community (DMLC).

to install : pip install xgboost

The main aim of this algorithm is to increase the speed and efficiency of computation. 
The Gradient Descent Boosting algorithm computes the output at a slower rate since they sequentially analyze the data set, 
therefore XGBoost is used to boost or extremely boost the performance of the model.


Gradient Boost Steps
Step1 - Calculate the mean and MSE
Step2 - Apply base learner 
Step3 - Calculate mean and MSE for each leaf 
Step4 - Calculate residue (value - mean) for each date point
Step5 - repeat 2,3,4
Step6 - Calculate the sum of means
Step7 - Calculate the final residue


System Features
The library provides a system for use in a range of computing environments, not least:

Parallelization of tree construction using all of your CPU cores during training.
Distributed Computing for training very large models using a cluster of machines.
Out-of-Core Computing for very large datasets that donâ€™t fit into memory.
Cache Optimization of data structures and algorithm to make best use of hardware.
Algorithm Features
The implementation of the algorithm was engineered for efficiency of compute time and memory resources. A design goal was to make the best use of available resources to train the model. Some key algorithm implementation features include:

Sparse Aware implementation with automatic handling of missing data values.
Block Structure to support the parallelization of tree construction.
Continued Training so that you can further boost an already fitted model on new data.
XGBoost is free open source software available for use under the permissive Apache-2 license.


https://www.madrasresearch.org/post/xgboost